<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Statistical Physics and Machine Learning (PSL Weeks 2025/2026) | Statistical Physics and Machine Learning</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="/assets/css/style.css">
</head>
<body>
  


<nav class="navbar">
  <a class="nav-title" href="/">PSL Week StatPhys&ML</a>

  <div class="nav-links">
    <div class="dropdown">
      <button class="dropbtn">Past Editions ▾</button>
      <div class="dropdown-content">
        
          
            <a class="active" href="/editions/2025-2026/">2025-2026 (current)</a>
          
        
          
            <a href="/editions/2024-2025/">2024/2025</a>
          
        
          
            <a href="/editions/2023-2024/">2023/2024</a>
          
        
          
            <a href="/editions/2022-2023/">2022/2023</a>
          
        
      </div>
    </div>
  </div>
</nav>

  <main class="container">
    <h1 id="20252026-psl-week-on-statistical-physics-and-machine-learning">2025/2026 PSL Week on <em>Statistical Physics and Machine Learning</em></h1>

<p>The past decade has witnessed a surge in the development and adoption of machine learning algorithms to solve day-a-day computational tasks. Yet, a solid theoretical understanding of even the most basic tools used in practice is still lacking, as traditional statistical learning methods are unfit to deal with the modern regime in which the number of model parameters are of the same order as the quantity of data – a problem known as the curse of dimensionality. Curiously, this is precisely the regime studied by Physicists since the mid 19th century in the context of interacting many-particle systems. This connection, which was first established in the seminal work of Elisabeth Gardner and Bernard Derrida in the 80s, is the basis of a long and fruitful marriage between these two fields.</p>

<p>The goal of this PSL week is to provide an in-depth overview of these connections and a good vision of the different tools available in the statistical physics toolbox, as well as their scope and limitations.</p>

<h2 id="course-information">Course Information</h2>

<ul>
  <li><strong>Instructors</strong>: <a href="https://brloureiro.github.io/">Bruno Loureiro</a> (CNRS &amp; DI-ENS) and <a href="https://anmaillard.github.io/">Antoine Maillard</a> (INRIA &amp; DI-ENS)</li>
  <li><strong>Dates</strong>: 11-15 April, 2022</li>
  <li><strong>Location</strong>: See PSL ENT or contact us.</li>
</ul>

<h2 id="evaluation">Evaluation</h2>

<p>Assiduity + Paper review.</p>

<h2 id="course-description">Course Description</h2>

<ul>
  <li>Historical background and introduction to statistical physics of learning.</li>
  <li>Statistical-to-computational gaps in high-dimensional inference: the spiked Wigner model, BBP transition, AMP, typical case complexity.</li>
  <li>High-dimensional theory for one-pass SGD: classical rates for convex optimization, scaling limits in high-dimensions, convergence rates for non-convex cases (the information exponent)</li>
  <li>Non-convex landscapes in high-dimensions: the Kac-Rice formula</li>
</ul>

<h2 id="requirements">Requirements</h2>

<p>Basic probability theory, linear algebra and analysis. No background in statistical physics will be assumed.</p>

<h2 id="recommended-literature">Recommended literature</h2>

<ul>
  <li>For the BBP transition and Kac-Rice, Antoine’s <a href="(https://anmaillard.github.io/assets/pdf/lecture_notes/2026_01_29_Orsay.pdf)">lecture notes</a> on <em>Statistical and computational phase
transitions in high-dimensional statistics</em></li>
  <li>Classical SGD theory, Chapter 5 in Francis’ <a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf">book</a>.</li>
  <li>For one-pass SGD, Bruno’s <a href="https://brloureiro.github.io/assets/pdf/psl_week_notes_sgd.pdf">lecture notes</a>.</li>
  <li>To go further, Bruno’s <a href="https://brloureiro.github.io/assets/pdf/NotesPrinceton_BL.pdf">lecture notes</a> on <em>Four lectures on Statistical Physics of Learning</em>.</li>
</ul>

<h2 id="course-schedule">Course Schedule</h2>

<table>
  <thead>
    <tr>
      <th>Time</th>
      <th>Monday 02/03</th>
      <th>Tuesday 03/03</th>
      <th>Wednesday 04/03</th>
      <th>Thursday 05/03</th>
      <th>Friday 06/03</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>10:00 - 12:00</td>
      <td>Introduction (Bruno)</td>
      <td>Spiked matrix I (Antoine)</td>
      <td>Spiked matrix III (Antoine)</td>
      <td>SGD II (Bruno)</td>
      <td>Kac-Rice II (Antoine)</td>
    </tr>
    <tr>
      <td>12:00 - 13:30</td>
      <td><strong>Lunch</strong></td>
      <td><strong>Lunch</strong></td>
      <td><strong>Lunch</strong></td>
      <td><strong>Lunch</strong></td>
      <td><strong>Lunch</strong></td>
    </tr>
    <tr>
      <td>13:30 - 15:30</td>
      <td>Denoising (Bruno)</td>
      <td>Spiked matrix II (Antoine)</td>
      <td>SGD I (Bruno)</td>
      <td>Kac-Rice I (Antoine)</td>
      <td>Seminar (TBC)</td>
    </tr>
  </tbody>
</table>


  </main>
</body>
</html>
